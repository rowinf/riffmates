"""
This type stub file was generated by pyright.
"""

"""
Multi-part parsing for file uploads.

Exposes one class, ``MultiPartParser``, which feeds chunks of uploaded data to
file upload handlers for processing.
"""
__all__ = ("MultiPartParser", "MultiPartParserError", "InputStreamExhausted")
class MultiPartParserError(Exception):
    ...


class InputStreamExhausted(Exception):
    """
    No more reads are allowed from this device.
    """
    ...


RAW = ...
FILE = ...
FIELD = ...
FIELD_TYPES = ...
class MultiPartParser:
    """
    An RFC 7578 multipart/form-data parser.

    ``MultiValueDict.parse()`` reads the input stream in ``chunk_size`` chunks
    and returns a tuple of ``(MultiValueDict(POST), MultiValueDict(FILES))``.
    """
    boundary_re = ...
    def __init__(self, META, input_data, upload_handlers, encoding=...) -> None:
        """
        Initialize the MultiPartParser object.

        :META:
            The standard ``META`` dictionary in Django request objects.
        :input_data:
            The raw post data, as a file-like object.
        :upload_handlers:
            A list of UploadHandler instances that perform operations on the
            uploaded data.
        :encoding:
            The encoding with which to treat the incoming data.
        """
        ...
    
    def parse(self): # -> tuple[QueryDict, MultiValueDict] | tuple[Any, Any]:
        ...
    
    def handle_file_complete(self, old_field_name, counters): # -> None:
        """
        Handle all the signaling that takes place when a file is complete.
        """
        ...
    
    def sanitize_file_name(self, file_name): # -> LiteralString | None:
        """
        Sanitize the filename of an upload.

        Remove all possible path separators, even though that might remove more
        than actually required by the target system. Filenames that could
        potentially cause problems (current/parent dir) are also discarded.

        It should be noted that this function could still return a "filepath"
        like "C:some_file.txt" which is handled later on by the storage layer.
        So while this function does sanitize filenames to some extent, the
        resulting filename should still be considered as untrusted user input.
        """
        ...
    
    IE_sanitize = ...


class LazyStream:
    """
    The LazyStream wrapper allows one to get and "unget" bytes from a stream.

    Given a producer object (an iterator that yields bytestrings), the
    LazyStream object will support iteration, reading, and keeping a "look-back"
    variable in case you need to "unget" some bytes.
    """
    def __init__(self, producer, length=...) -> None:
        """
        Every LazyStream must have a producer when instantiated.

        A producer is an iterable that returns a string each time it
        is called.
        """
        ...
    
    def tell(self): # -> int:
        ...
    
    def read(self, size=...): # -> bytes:
        ...
    
    def __next__(self): # -> bytes:
        """
        Used when the exact number of bytes to read is unimportant.

        Return whatever chunk is conveniently returned from the iterator.
        Useful to avoid unnecessary bookkeeping if performance is an issue.
        """
        ...
    
    def close(self): # -> None:
        """
        Used to invalidate/disable this lazy stream.

        Replace the producer with an empty list. Any leftover bytes that have
        already been read will still be reported upon read() and/or next().
        """
        ...
    
    def __iter__(self): # -> Self:
        ...
    
    def unget(self, bytes): # -> None:
        """
        Place bytes back onto the front of the lazy stream.

        Future calls to read() will return those bytes first. The
        stream position and thus tell() will be rewound.
        """
        ...
    


class ChunkIter:
    """
    An iterable that will yield chunks of data. Given a file-like object as the
    constructor, yield chunks of read operations from that object.
    """
    def __init__(self, flo, chunk_size=...) -> None:
        ...
    
    def __next__(self):
        ...
    
    def __iter__(self): # -> Self:
        ...
    


class InterBoundaryIter:
    """
    A Producer that will iterate over boundaries.
    """
    def __init__(self, stream, boundary) -> None:
        ...
    
    def __iter__(self): # -> Self:
        ...
    
    def __next__(self): # -> LazyStream:
        ...
    


class BoundaryIter:
    """
    A Producer that is sensitive to boundaries.

    Will happily yield bytes until a boundary is found. Will yield the bytes
    before the boundary, throw away the boundary bytes themselves, and push the
    post-boundary bytes back on the stream.

    The future calls to next() after locating the boundary will raise a
    StopIteration exception.
    """
    def __init__(self, stream, boundary) -> None:
        ...
    
    def __iter__(self): # -> Self:
        ...
    
    def __next__(self): # -> bytes:
        ...
    


def exhaust(stream_or_iterable): # -> None:
    """Exhaust an iterator or stream."""
    ...

def parse_boundary_stream(stream, max_header_size): # -> tuple[Literal['raw'], dict[Any, Any], Any] | tuple[Literal['raw', 'file', 'field'], dict[Any, Any], Any]:
    """
    Parse one and exactly one stream that encapsulates a boundary.
    """
    ...

class Parser:
    def __init__(self, stream, boundary) -> None:
        ...
    
    def __iter__(self): # -> Generator[tuple[Literal['raw'], dict[Any, Any], Any] | tuple[Literal['raw', 'file', 'field'], dict[Any, Any], Any], Any, None]:
        ...
    


